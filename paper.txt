2스트림 네트워크를 사용: RGB 영상 (공간 정보) / Optical Flow (시간 정보)
자기 지도 학습(Self-Supervised Learning, SSL) 방식 채택: 사람이 라벨링하지 않은 데이터를 활용해 성능 저하 없이 효율을 높이는 방식입니다.
VICReg 기반 SSL 적용: 기존 contrastive 방법들과 달리 대규모 데이터나 메모리 뱅크 없이도 안정적으로 임베딩을 학습.

**VICReg (Variance-Invariance-Covariance Regularization)**은 Meta AI (LeCun 외)에 의해 제안된 자기 지도 학습(self-supervised learning, SSL) 기법
기존 contrastive learning 기법과는 다르게, **음성 쌍(negative pairs)**이나 대규모 메모리 뱅크, 큐 구조 없이도 좋은 임베딩을 학습할 수 있도록 설계

📌 VICReg의 목적 → 좋은 표현(embedding)을 얻기 위한 세 가지 제약 조건을 손실 함수로 정의:
Invariance (불변성): 서로 다른 view (augmentation)라도 같은 의미의 데이터를 비슷한 벡터로 표현되어야 함 → 두 임베딩 벡터 간 거리 최소화
Variance (다양성): 모든 데이터가 같은 임베딩이 되지 않도록 표현의 다양성을 보장 → 임베딩의 표준편차가 0이 되지 않게 유지
Covariance (비상관성): 서로 다른 feature 차원이 중복 정보를 갖지 않도록 → 임베딩의 feature 간 상관관계를 줄임

Spatial Block (RGB 영상 처리 모듈)
- 입력 영상에서 **중요한 공간 정보(물체, 인물, 배경, 자세 등)**를 추출하여 폭력 징후를 파악
- 특히 움직임이 많은 영역만 선택(ROI)하여, 연산량을 줄이면서도 의미 있는 정보에 집중
1. Optical Flow 기반 ROI 계산
2. ROI 추출 (112×112 패치)
3. Cubic Interpolation → 224×224 리사이즈
4. 3D Conv → BatchNorm → ReLU
5. 출력 텐서 생성: 3 × N × 224 × 224

Temporal Block (시간 정보 처리 모듈)
- Optical Flow를 이용해 프레임 간 움직임을 분석, RGB가 놓치는 "시간 축의 정보"를 보완하는 역할
- 사람의 행동 중에서도 **"움직임의 강도, 방향성, 패턴"**에 주목하여 폭력 여부를 탐지
  (Gunnar Farneback 알고리즘: 고속이며 dense optical flow를 계산할 수 있어 감시 영상에 적합.)
1. Optical Flow 계산 (Gunnar Farneback)
2. N개의 연속된 Optical Flow 프레임 생성
3. 3D Convolution → BatchNorm → Sigmoid
4. RGB 스트림과 결합(Merging Block에서 사용)

Merging Block (Flow-Gated Fusion Layer)
- RGB 스트림과 Optical Flow 스트림의 출력을 **정보적으로 결합(fusion)**하여 불필요한 공간 정보는 제거, 의미 있는 움직임 중심의 정보는 강조
- Optical Flow 출력을 **게이트(gate)**처럼 사용해 RGB에서 유의미한 부분만 유지
- 두 스트림의 출력을 요소별 곱(element-wise multiplication) 한 후, Temporal Max Pooling을 적용해 중요한 시간적 특징만 남기는 방식
1. RGB Block 출력: R ∈ ℝ^{C×T×H×W}
2. Flow Block 출력: F ∈ ℝ^{C×T×H×W}
3. Gated Fusion: M = R ⊙ F
4. Temporal Max Pooling: M′ = MaxPool_T(M)
5. FC Layer → Sigmoid → Binary Output

Auxiliary Self-Supervised Model: VICReg for Video Streams(VICReg 구조)
VICReg의 기본 철학을 **멀티모달 비디오 데이터 (RGB + Optical Flow)**에 맞게 확장하여, **효율적이고 강건한 임베딩(embedding)**을 라벨 없이(self-supervised) 학습
이를 기반으로 **Primary Model (FGN)**의 성능을 극대화하기 위한 사전학습 프레임워크

[RGB 입력 I]      [Optical Flow 입력 I′]    하나의 비디오 세그먼트에서 RGB 시퀀스 I와 **Optical Flow 시퀀스 I′**를 추출(두 입력은 동일한 시점의 서로 다른 modality)
     ↓                   ↓
Augmentation        Augmentation            RGB 입력 I → 강한 랜덤 Zoom Crop + Color Jitter 등 / Flow 입력 I′ → **수평 뒤집기 (Horizontal Flip)**만 사용
     ↓                   ↓
   View X              View X′              
     ↓                   ↓
Encoder fθ         Encoder f′θ′             RGB용 인코더와 Flow용 인코더는 서로 다른 구조와 파라미터를 사용 (Non-sharing Siamese 구조) / 이 인코더는 기본적으로 3D Convolution 기반의 시간-공간 특징 추출기
     ↓                   ↓
Siamese Merging Block mγ (공통 구조, 서로 다른 입력)    RGB와 Flow 각각의 인코더 출력을 병합하는 블록, FGN의 merging block 구조를 그대로 사용하지만, Temporal Max Pooling은 제거
     ↓                   ↓
Expander hϕ         Expander hϕ             3개의 Fully Connected Layer (크기: 8192), 앞 두 층은 BatchNorm + ReLU 포함
     ↓                   ↓
 Embedding Z         Embedding Z′
          ↘           ↙
         VICReg 손실 계산 (L(Z, Z′))            VICReg 손실 계산